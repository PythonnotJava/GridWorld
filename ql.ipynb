{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-28T06:44:22.268204Z",
     "start_time": "2024-09-28T06:44:22.249592Z"
    }
   },
   "source": [
    "from typing import *\n",
    "\n",
    "import random\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "# 定义环境\n",
    "class GridWorld:\n",
    "    def __init__(\n",
    "            self,\n",
    "            rows : int,\n",
    "            cols : int,\n",
    "            start : Tuple[int, int],\n",
    "            goal : Tuple[int, int],\n",
    "            obstacles : List[Tuple[int, int]]\n",
    "    ):\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.start = start\n",
    "        self.goal = goal\n",
    "        self.obstacles = obstacles\n",
    "        self.state = start\n",
    "\n",
    "    def reset(self) -> Tuple[int, int]:\n",
    "        self.state = self.start\n",
    "        return self.state\n",
    "\n",
    "    def move(self, action : int) -> Tuple:\n",
    "        next_state = None\n",
    "        if action == 0:  # 上\n",
    "            next_state = (self.state[0] - 1, self.state[1])\n",
    "        elif action == 1:  # 下\n",
    "            next_state = (self.state[0] + 1, self.state[1])\n",
    "        elif action == 2:  # 左\n",
    "            next_state = (self.state[0], self.state[1] - 1)\n",
    "        elif action == 3:  # 右\n",
    "            next_state = (self.state[0], self.state[1] + 1)\n",
    "\n",
    "        # 考虑碰到边缘和障碍物的情况\n",
    "        if (0 <= next_state[0] < self.rows and\n",
    "                0 <= next_state[1] < self.cols and\n",
    "                next_state not in self.obstacles):\n",
    "            self.state = next_state\n",
    "\n",
    "        # 到达终点\n",
    "        if self.state == self.goal:\n",
    "            return self.state, 1, True  # 到达目标，奖励1\n",
    "        return self.state, -0.1, False  # 每步惩罚\n",
    "\n",
    "# QLearning算法\n",
    "class QLearningAgent:\n",
    "    def __init__(\n",
    "            self,\n",
    "            actions : List[int],\n",
    "            goal : List[int],\n",
    "            alpha : Annotated[float, \"学习率\"]=0.1,\n",
    "            gamma : Annotated[float, '折扣因子']=0.9,\n",
    "            epsilon : Annotated[float, '探索率']=0.1\n",
    "    ):\n",
    "        self.q_table = {}  # key=(state, action) value=float\n",
    "        self.actions = actions\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.goal = goal\n",
    "\n",
    "    def get_q_value(self, state : Tuple[int, int], action : int) -> float:\n",
    "        return self.q_table.get((state, action), 0.0)\n",
    "\n",
    "    # 采用e-g策略，增加灵活性\n",
    "    def choose_action(self, state : Tuple[int, int]) -> int:\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return random.choice(self.actions)  # 探索\n",
    "        else:\n",
    "            q_values = [self.get_q_value(state, a) for a in self.actions]\n",
    "            max_q = max(q_values)\n",
    "            return self.actions[q_values.index(max_q)]  # 利用\n",
    "\n",
    "    def update_q_value(\n",
    "            self,\n",
    "            state: Tuple[int, int],\n",
    "            action: int,\n",
    "            reward: float,\n",
    "            next_state: Tuple[int, int]\n",
    "    ) -> None:\n",
    "        if next_state == self.goal:\n",
    "            # 如果到达终点，将Q值设为当前奖励\n",
    "            self.q_table[(state, action)] = (self.get_q_value(state, action) +\n",
    "                                             self.alpha * (reward - self.get_q_value(state, action)))\n",
    "        else:\n",
    "            best_next_q = max([self.get_q_value(next_state, a) for a in self.actions])\n",
    "            self.q_table[(state, action)] = (self.get_q_value(state, action) +\n",
    "                                             self.alpha * (reward + self.gamma * best_next_q - self.get_q_value(state, action)))\n",
    "\n",
    "# 主程序\n",
    "def main():\n",
    "    rows = 5\n",
    "    cols = 6  # 支持任意列数\n",
    "    start = (0, 0)\n",
    "    goal = (4, 5)\n",
    "    obstacles = [(1, 1), (2, 1), (3, 4), (4, 3), (0, 3)]\n",
    "\n",
    "    env = GridWorld(rows, cols, start, goal, obstacles)\n",
    "    agent = QLearningAgent(actions=[0, 1, 2, 3], goal=[4, 5])  # 上、下、左、右\n",
    "\n",
    "    episodes = 1\n",
    "    for episode in range(1, 1 + episodes):\n",
    "        state = env.reset()\n",
    "        print(f\"第{episode}轮开始！\")\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, done = env.move(action)\n",
    "            agent.update_q_value(state, action, reward, next_state)\n",
    "            state = next_state\n",
    "\n",
    "    # 打印学习到的Q值\n",
    "    return agent.q_table\n",
    "    # for state_action, value in agent.q_table.items():\n",
    "    #     print(f\"State: {state_action[0]}, Action: {state_action[1]}, Q-value: {value:.2f}\")\n",
    "\n",
    "    \n",
    "display(main())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1轮开始！\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{((0, 0), 0): -0.0199,\n",
       " ((0, 0), 1): -0.028000000000000004,\n",
       " ((1, 0), 0): -0.02962,\n",
       " ((0, 0), 2): -0.0199,\n",
       " ((0, 0), 3): -0.028810000000000002,\n",
       " ((0, 1), 0): -0.0199,\n",
       " ((0, 1), 1): -0.0199,\n",
       " ((0, 1), 2): -0.021520000000000004,\n",
       " ((1, 0), 1): -0.028000000000000004,\n",
       " ((2, 0), 0): -0.029701000000000005,\n",
       " ((1, 0), 2): -0.02962,\n",
       " ((1, 0), 3): -0.0199,\n",
       " ((0, 1), 3): -0.028810000000000002,\n",
       " ((0, 2), 0): -0.0199,\n",
       " ((0, 2), 1): -0.028000000000000004,\n",
       " ((1, 2), 0): -0.0199,\n",
       " ((0, 2), 2): -0.020791000000000004,\n",
       " ((0, 2), 3): -0.0199,\n",
       " ((1, 2), 1): -0.028000000000000004,\n",
       " ((2, 2), 0): -0.0199,\n",
       " ((1, 2), 2): -0.0199,\n",
       " ((1, 2), 3): -0.019000000000000003,\n",
       " ((1, 3), 0): -0.0199,\n",
       " ((1, 3), 1): -0.019000000000000003,\n",
       " ((2, 3), 0): -0.0199,\n",
       " ((1, 3), 2): -0.021520000000000004,\n",
       " ((2, 0), 1): -0.010000000000000002,\n",
       " ((3, 0), 0): -0.010000000000000002,\n",
       " ((2, 0), 2): -0.010000000000000002,\n",
       " ((2, 0), 3): -0.010000000000000002,\n",
       " ((2, 2), 1): -0.019000000000000003,\n",
       " ((3, 2), 3): -0.010000000000000002,\n",
       " ((3, 3), 0): -0.0199,\n",
       " ((2, 3), 1): -0.019000000000000003,\n",
       " ((3, 3), 1): -0.010000000000000002,\n",
       " ((3, 3), 2): -0.010000000000000002,\n",
       " ((3, 2), 0): -0.0199,\n",
       " ((2, 2), 2): -0.0199,\n",
       " ((2, 2), 3): -0.0199,\n",
       " ((2, 3), 2): -0.010900000000000002,\n",
       " ((1, 3), 3): -0.019000000000000003,\n",
       " ((1, 4), 0): -0.019000000000000003,\n",
       " ((0, 4), 0): -0.0199,\n",
       " ((0, 4), 1): -0.0199,\n",
       " ((1, 4), 1): -0.019000000000000003,\n",
       " ((2, 4), 0): -0.0199,\n",
       " ((1, 4), 2): -0.020791000000000004,\n",
       " ((2, 3), 3): -0.0199,\n",
       " ((2, 4), 1): -0.0199,\n",
       " ((2, 4), 2): -0.010900000000000002,\n",
       " ((1, 4), 3): -0.019000000000000003,\n",
       " ((1, 5), 0): -0.0199,\n",
       " ((0, 5), 1): -0.0199,\n",
       " ((1, 5), 1): -0.019000000000000003,\n",
       " ((2, 5), 0): -0.0199,\n",
       " ((1, 5), 2): -0.020791000000000004,\n",
       " ((0, 4), 2): -0.019000000000000003,\n",
       " ((0, 4), 3): -0.019000000000000003,\n",
       " ((0, 5), 0): -0.0199,\n",
       " ((0, 5), 2): -0.020710000000000003,\n",
       " ((0, 5), 3): -0.010000000000000002,\n",
       " ((2, 4), 3): -0.0199,\n",
       " ((2, 5), 1): -0.019000000000000003,\n",
       " ((3, 5), 0): -0.010000000000000002,\n",
       " ((2, 5), 2): -0.010900000000000002,\n",
       " ((1, 5), 3): -0.0199,\n",
       " ((2, 5), 3): -0.010000000000000002,\n",
       " ((3, 2), 1): -0.010000000000000002,\n",
       " ((4, 2), 0): -0.010000000000000002,\n",
       " ((3, 2), 2): -0.010000000000000002,\n",
       " ((3, 1), 0): -0.010000000000000002,\n",
       " ((3, 1), 1): -0.010000000000000002,\n",
       " ((4, 1), 0): -0.010000000000000002,\n",
       " ((3, 1), 2): -0.010000000000000002,\n",
       " ((3, 0), 2): -0.010000000000000002,\n",
       " ((3, 0), 1): -0.010000000000000002,\n",
       " ((4, 0), 0): -0.010000000000000002,\n",
       " ((3, 0), 3): -0.010000000000000002,\n",
       " ((3, 1), 3): -0.010900000000000002,\n",
       " ((3, 3), 3): -0.010000000000000002,\n",
       " ((3, 5), 1): 0.1}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
